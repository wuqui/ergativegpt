{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Markdown\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import altair as alt\n",
    "\n",
    "# Import our custom library functions\n",
    "from ergativegpt.data import read_linguistic_data, load_experiment_data\n",
    "from ergativegpt.config import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment configuration\n",
    "config = load_config('experiments/config/2025-06-23.yaml')\n",
    "print(f\"Experiment: {config.name}\")\n",
    "print(f\"Description: {config.description}\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Target: {config.target_verb}\")\n",
    "\n",
    "# Create output directory\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using library function\n",
    "fp = Path(config.input_dir) / config.target_file\n",
    "print(f\"Loading data from: {fp}\")\n",
    "\n",
    "df = read_linguistic_data(fp)\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fp in fps:\n",
    "#     read_data(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    config.model_name, \n",
    "    model_provider=config.model_provider\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal\n",
    "\n",
    "class UtteranceClassification(BaseModel):\n",
    "    \"\"\"Classifying verbs according to several linguistic criteria.\"\"\"\n",
    "    gpt_transitivity: Literal['transitive', 'intransitive'] = Field(..., description=\"Whether this use of the verb should be labelled either `transitive` or `intransitive`.\")\n",
    "    gpt_causativity: Literal['causative', 'anticausative'] = Field(..., description=\"Whether this use of the verb should be labelled as `causative` or `anticausative`.\")\n",
    "    gpt_subject_role: Literal['agent', 'patient'] = Field(..., description='Whether the semantic role of the subject of the verb should be labelled as either `agent` or `patient`.')\n",
    "    gpt_subject_animacy: Literal['animate', 'inanimate'] = Field(..., description='Whether the subject of the verb should be labelled as `animate` or `inanimate`.')\n",
    "    gpt_verb_pos: Literal['verb', 'other'] = Field(..., description='Whether the attestation is a verb or not. Label it `verb` or `other`.')\n",
    "    gpt_verb_voice: Literal['active', 'passive'] = Field(..., description='Whether the verb is used in active or passive voice.')\n",
    "    gpt_subject: str = Field(..., description='The subject in this utterance that you considered for your classification.')\n",
    "    gpt_verb: str = Field(..., description='The verb in this utterance that you considered for your classification.')\n",
    "    gpt_object: str = Field(..., description='The object in this utterance that you considered for your classification.')\n",
    "\n",
    "structured_llm = llm.with_structured_output(UtteranceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.prompt_file, 'r') as f:\n",
    "        prompt_txt = f.read()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', prompt_txt),\n",
    "        ('human', 'Please classify the following sentence: {input}')\n",
    "])\n",
    "\n",
    "# Markdown(prompt_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- done\n",
    "    - develop\n",
    "    - dry\n",
    "    - open\n",
    "    - wake\n",
    "    - freeze\n",
    "    - fill\n",
    "    - close\n",
    "    - dissolve\n",
    "    - split\n",
    "    - improve\n",
    "- doing\n",
    "- todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../in/2025-04-14/COHA_develop.xlsx\n",
      "develop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(180, 15)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb = config.target_verb\n",
    "print(f\"Processing verb: {verb}\")\n",
    "print(f\"Data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10c0562e8304241808fbda74b434df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dev = (df\n",
    "    #   .head(5)\n",
    "      )\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for _, row in tqdm(df_dev.iterrows(), total=len(df_dev)):\n",
    "    try:\n",
    "        result = chain.invoke({\"input\": verb + ' in: ' + row['text']})\n",
    "        row_data = {\n",
    "            'ID': row['ID'],\n",
    "            'text': row['text'],\n",
    "\n",
    "            'year': row['year'],\n",
    "            'period': row['period'],\n",
    "            'genre': row['genre'],\n",
    "            'source': row['source'],\n",
    "            'ambiguous_target_verb': row['ambiguous_target_verb'],\n",
    "\n",
    "            'transitivity': row['transitivity'],\n",
    "            'gpt_transitivity': result.gpt_transitivity,\n",
    "            'causativity': row['causativity'],\n",
    "            'gpt_causativity': result.gpt_causativity,\n",
    "            'subject_animacy': row['subject_animacy'],\n",
    "            'gpt_subject_animacy': result.gpt_subject_animacy,\n",
    "            'subject_role': row['subject_role'],\n",
    "            'gpt_subject_role': result.gpt_subject_role,\n",
    "            'pos': row['pos'],\n",
    "            'gpt_pos': result.gpt_verb_pos,\n",
    "\n",
    "            'gpt_verb_voice': result.gpt_verb_voice,\n",
    "            'gpt_subject': result.gpt_subject,\n",
    "            'gpt_verb': result.gpt_verb,\n",
    "            'gpt_object': result.gpt_object\n",
    "        }\n",
    "        results_list.append(row_data)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing row:\", e)\n",
    "        continue\n",
    "\n",
    "results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'develop'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 18)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(out_dir / f'{verb}_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_inspect = (results\n",
    "    # .query('causativity != gpt_causativity')\n",
    "    .filter(['text', \n",
    "            'causativity', 'gpt_causativity',\n",
    "            'transitivity', 'gpt_transitivity',\n",
    "            'gpt_verb_voice',\n",
    "            'gpt_subject', 'gpt_verb', 'gpt_object'\n",
    "            ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_26754/3627494057.py:36: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  present_labels = sorted(list(pd.unique(y_true.astype(str).tolist() + y_pred.astype(str).tolist())))\n",
      "/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_26754/3627494057.py:36: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  present_labels = sorted(list(pd.unique(y_true.astype(str).tolist() + y_pred.astype(str).tolist())))\n",
      "/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_26754/3627494057.py:36: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  present_labels = sorted(list(pd.unique(y_true.astype(str).tolist() + y_pred.astype(str).tolist())))\n",
      "/var/folders/gp/dw55jb3d3gl6jn22rscvxjm40000gn/T/ipykernel_26754/3627494057.py:36: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  present_labels = sorted(list(pd.unique(y_true.astype(str).tolist() + y_pred.astype(str).tolist())))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>causativity</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject_role</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject_animacy</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               var  accuracy  precision  recall    f1  support_pos\n",
       "0     transitivity      0.95       0.94    0.96  0.95           77\n",
       "1      causativity      0.93       0.81    0.96  0.88           48\n",
       "2     subject_role      0.90       0.87    0.89  0.88           74\n",
       "3  subject_animacy      0.93       0.92    0.92  0.92           77"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = []\n",
    "vars = {\n",
    "    'transitivity': {'average': None, 'pos_label': 'intransitive'},\n",
    "    'causativity': {'average': None, 'pos_label': 'anticausative'},\n",
    "    'subject_role': {'average': None, 'pos_label': 'patient'},\n",
    "    'subject_animacy': {'average': None, 'pos_label': 'inanimate'}\n",
    "}\n",
    "\n",
    "# Keep a reference to the original results before the loop\n",
    "original_results = results.copy()\n",
    "\n",
    "for var, settings in vars.items():\n",
    "    # --- Filtering Step ---\n",
    "    # Filter the *original* DataFrame for the current variable, creating a temporary view\n",
    "    filtered_results = original_results.dropna(subset=[var, f'gpt_{var}'])\n",
    "\n",
    "    # --- Data Extraction ---\n",
    "    y_true = filtered_results[var]\n",
    "    y_pred = filtered_results[f'gpt_{var}']\n",
    "\n",
    "    # --- Handle Empty Data ---\n",
    "    if len(y_true) == 0:\n",
    "        print(f\"Skipping {var}: No valid samples after removing NaNs.\")\n",
    "        metrics.append({\n",
    "            'var': var,\n",
    "            'accuracy': np.nan,\n",
    "            'precision': np.nan,\n",
    "            'recall': np.nan,\n",
    "            'f1': np.nan,\n",
    "            'support_pos': 0,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # --- Label Handling ---\n",
    "    # Determine the unique labels present in the *filtered* data and sort them\n",
    "    present_labels = sorted(list(pd.unique(y_true.astype(str).tolist() + y_pred.astype(str).tolist())))\n",
    "\n",
    "    # --- Metric Calculation ---\n",
    "    # Calculate overall accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate metrics per class, ensuring zero_division handling\n",
    "    # Pass the sorted list of labels actually present in the filtered data\n",
    "    p, r, f, s = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        average=None, # Crucial: Get metrics per class\n",
    "        labels=present_labels, # Ensure metrics align with these labels\n",
    "        zero_division=0 # Handle cases with no true/predicted samples for a class\n",
    "    )\n",
    "\n",
    "    # --- Extract Metrics for Positive Class ---\n",
    "    # Check if the desired positive label exists in the filtered data\n",
    "    if settings['pos_label'] in present_labels:\n",
    "        # Find the index of the positive label in the *sorted* list\n",
    "        pos_label_index = present_labels.index(settings['pos_label'])\n",
    "        precision_pos = p[pos_label_index]\n",
    "        recall_pos = r[pos_label_index]\n",
    "        f1_pos = f[pos_label_index]\n",
    "        support_pos = s[pos_label_index]\n",
    "    else:\n",
    "        # Positive label not found in this subset after filtering NaNs\n",
    "        print(f\"Warning for {var}: Positive label '{settings['pos_label']}' not found in filtered ground truth. Precision/Recall/F1/Support for this class are undefined.\")\n",
    "        precision_pos, recall_pos, f1_pos, support_pos = np.nan, np.nan, np.nan, 0\n",
    "\n",
    "    # --- Append Results ---\n",
    "    metrics.append({\n",
    "        'var': var,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision_pos,\n",
    "        'recall': recall_pos,\n",
    "        'f1': f1_pos,\n",
    "        'support_pos': int(support_pos), # Ensure support is an integer\n",
    "    })\n",
    "\n",
    "# --- Final DataFrame ---\n",
    "metrics_df = pd.DataFrame(metrics).round(2)\n",
    "metrics_df # Display the resulting DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "metrics_df = (metrics_df\n",
    "\t.drop('support_pos', axis=1)\n",
    "\t.melt(id_vars=['var'], var_name='metric', value_name='value')\n",
    "\t.round(2)\n",
    "\t.sort_values(['var', 'metric'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-b1c2407df01e45acac8cc06bd1a9359d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-b1c2407df01e45acac8cc06bd1a9359d.vega-embed details,\n",
       "  #altair-viz-b1c2407df01e45acac8cc06bd1a9359d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-b1c2407df01e45acac8cc06bd1a9359d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-b1c2407df01e45acac8cc06bd1a9359d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-b1c2407df01e45acac8cc06bd1a9359d\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-991b5e01bb806ad37fa6af9cf014b2e6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"metric\", \"legend\": null, \"type\": \"nominal\"}, \"column\": {\"field\": \"var\", \"title\": \"\", \"type\": \"nominal\"}, \"x\": {\"field\": \"metric\", \"sort\": [\"accuracy\", \"precision\", \"recall\", \"f1\"], \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"value\", \"title\": \"\", \"type\": \"quantitative\"}}, \"title\": \"Metrics for \\u201cdevelop\\u201d\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-991b5e01bb806ad37fa6af9cf014b2e6\": [{\"var\": \"causativity\", \"metric\": \"accuracy\", \"value\": 0.93}, {\"var\": \"causativity\", \"metric\": \"f1\", \"value\": 0.88}, {\"var\": \"causativity\", \"metric\": \"precision\", \"value\": 0.81}, {\"var\": \"causativity\", \"metric\": \"recall\", \"value\": 0.96}, {\"var\": \"subject_animacy\", \"metric\": \"accuracy\", \"value\": 0.93}, {\"var\": \"subject_animacy\", \"metric\": \"f1\", \"value\": 0.92}, {\"var\": \"subject_animacy\", \"metric\": \"precision\", \"value\": 0.92}, {\"var\": \"subject_animacy\", \"metric\": \"recall\", \"value\": 0.92}, {\"var\": \"subject_role\", \"metric\": \"accuracy\", \"value\": 0.9}, {\"var\": \"subject_role\", \"metric\": \"f1\", \"value\": 0.88}, {\"var\": \"subject_role\", \"metric\": \"precision\", \"value\": 0.87}, {\"var\": \"subject_role\", \"metric\": \"recall\", \"value\": 0.89}, {\"var\": \"transitivity\", \"metric\": \"accuracy\", \"value\": 0.95}, {\"var\": \"transitivity\", \"metric\": \"f1\", \"value\": 0.95}, {\"var\": \"transitivity\", \"metric\": \"precision\", \"value\": 0.94}, {\"var\": \"transitivity\", \"metric\": \"recall\", \"value\": 0.96}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = alt.Chart(metrics_df).mark_bar().encode(\n",
    "\tx=alt.X('metric:N', title='', sort=['accuracy', 'precision', 'recall', 'f1']),\n",
    "\ty=alt.Y('value:Q', title=''),\n",
    "\tcolumn=alt.Column('var:N', title=''),\n",
    "\tcolor=alt.Color('metric:N', legend=None)\n",
    ").properties(title=f\"Metrics for “{verb}”\")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart.save(out_dir / f'{verb}_metrics_plot.png', scale_factor=3.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
