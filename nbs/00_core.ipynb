{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ergativegpt.src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support,  accuracy_score\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../in/Data_GPT.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"gpt-3.5-turbo\"\n",
    "# model=\"gpt-4\"\n",
    "\n",
    "llm = ChatOpenAI(model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../in/prompt.md', 'r') as f:\n",
    "        prompt_txt = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", prompt_txt),\n",
    "        (\"human\", 'Please classify the following sentence: {input}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class UtteranceClassification(BaseModel):\n",
    "    \"\"\"Classifying the utterances according to linguistic criteria.\"\"\"\n",
    "    gpt_transitivity: str = Field(..., description=\"Whether this use of the verb should be labelled either `transitive` or `intransitive`.\")\n",
    "    gpt_causativity: str = Field(..., description=\"Whether this use of the verb should be labelled as `causative` or `anticausative`.\")\n",
    "    gpt_subject_animacy: str = Field(..., description='Whether the subject of the verb should be labelled as `animate` or `inamimate`.')\n",
    "    gpt_subject_role: str = Field(..., description='Whether the semantic role of the subject of the verb should be labelled as either `agent` or `patient`.')\n",
    "    gpt_subject: str = Field(..., description='The subject in this utterance that you considered for your classification.')\n",
    "    gpt_verb: str = Field(..., description='The verb in this utterance that you considered for your classification.')\n",
    "    gpt_object: str = Field(..., description='The object in this utterance that you considered for your classification.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = create_structured_output_runnable(UtteranceClassification, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e53e2f71d44a68b9218f9f485d57e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "df_dev = (df\n",
    "    .sample(1)\n",
    ")\n",
    "\n",
    "for _, row in tqdm(df_dev.iterrows(), total=len(df_dev)):\n",
    "    try:\n",
    "        result = runnable.invoke({\"input\": row['verbRealization'] + ' in: ' + row['Token']})\n",
    "        row_data = {\n",
    "            'text': row['Token'],\n",
    "            'transitivity': row['Transitivity'],\n",
    "            'gpt_transitivity': result.gpt_transitivity,\n",
    "            'causativity': row['Construction'],\n",
    "            'gpt_causativity': result.gpt_causativity,\n",
    "            'subject_animacy': row['subjectAnimacy'],\n",
    "            'gpt_subject_animacy': result.gpt_subject_animacy,\n",
    "            'subject_role': row['subjectRole'],\n",
    "            'gpt_subject_role': result.gpt_subject_role,\n",
    "\n",
    "            'gpt_subject': result.gpt_subject,\n",
    "            'gpt_verb': result.gpt_verb,\n",
    "            'gpt_object': result.gpt_object\n",
    "        }\n",
    "        results_list.append(row_data)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['transitivity'] = results['transitivity'].replace({'Transitive': 'transitive', 'Intransitive': 'intransitive'})\n",
    "results['causativity'] = results['causativity'].replace({0: 'causative', 1: 'anticausative'})\n",
    "results['subject_animacy'] = results['subject_animacy'].replace({'Animate': 'animate', 'Inanimate': 'inanimate'})\n",
    "results['subject_role'] = results['subject_role'].replace({'Agent': 'agent', 'Patient': 'patient'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = {\n",
    "\t'transitivity': 'intransitive',\n",
    "\t'causativity': 'anticausative',\n",
    "\t'subject_animacy': 'inanimate',\n",
    "\t'subject_role': 'patient',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(results, variable, pos_label):\n",
    "\tgpt_variable = f'gpt_{variable}'\n",
    "\tprecision, recall, f1, support = precision_recall_fscore_support(\n",
    "\t\tresults[variable], results[gpt_variable], \n",
    "\t\tpos_label=pos_label,\n",
    "\t\taverage='binary',\n",
    "\t)\n",
    "\tmetrics = pd.DataFrame(\n",
    "\t\tcolumns = ['variable', 'metric', 'score'],\n",
    "\t\tdata = [\n",
    "\t\t\t[variable, 'precision', round(precision, 2)],\n",
    "\t\t\t[variable, 'recall', round(recall, 2)],\n",
    "\t\t\t[variable, 'accuracy', accuracy_score(results[variable], results[gpt_variable])],\n",
    "\t\t\t[variable, 'F1', round(f1, 2)]\n",
    "\t\t])\n",
    "\treturn metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ergativegpt/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics_vars = []\n",
    "for var in vars.items():\n",
    "\tmetrics_var = get_metrics(results, var[0], var[1])\n",
    "\tmetrics_vars.append(metrics_var)\n",
    "\n",
    "metrics = pd.concat(metrics_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>causativity</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>causativity</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>causativity</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>causativity</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject_animacy</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject_animacy</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject_animacy</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject_animacy</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject_role</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject_role</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject_role</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject_role</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variable     metric  score\n",
       "0     transitivity  precision    0.0\n",
       "1     transitivity     recall    0.0\n",
       "2     transitivity   accuracy    1.0\n",
       "3     transitivity         F1    0.0\n",
       "0      causativity  precision    0.0\n",
       "1      causativity     recall    0.0\n",
       "2      causativity   accuracy    1.0\n",
       "3      causativity         F1    0.0\n",
       "0  subject_animacy  precision    0.0\n",
       "1  subject_animacy     recall    0.0\n",
       "2  subject_animacy   accuracy    1.0\n",
       "3  subject_animacy         F1    0.0\n",
       "0     subject_role  precision    0.0\n",
       "1     subject_role     recall    0.0\n",
       "2     subject_role   accuracy    1.0\n",
       "3     subject_role         F1    0.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-d38259274fe144bd85bad10afe4e1d2f.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-d38259274fe144bd85bad10afe4e1d2f.vega-embed details,\n",
       "  #altair-viz-d38259274fe144bd85bad10afe4e1d2f.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-d38259274fe144bd85bad10afe4e1d2f\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d38259274fe144bd85bad10afe4e1d2f\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d38259274fe144bd85bad10afe4e1d2f\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.15.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.15.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-8cfdf1f86e5b38c580131ac46e51da9b\"}, \"facet\": {\"column\": {\"field\": \"variable\", \"type\": \"nominal\"}}, \"spec\": {\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"metric\", \"legend\": null, \"type\": \"nominal\"}, \"x\": {\"field\": \"metric\", \"sort\": [\"precision\", \"recall\", \"accuracy\", \"F1\", \"precision\", \"recall\", \"accuracy\", \"F1\", \"precision\", \"recall\", \"accuracy\", \"F1\", \"precision\", \"recall\", \"accuracy\", \"F1\"], \"type\": \"nominal\"}, \"y\": {\"field\": \"score\", \"type\": \"quantitative\"}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.15.1.json\", \"datasets\": {\"data-8cfdf1f86e5b38c580131ac46e51da9b\": [{\"variable\": \"transitivity\", \"metric\": \"precision\", \"score\": 0.0}, {\"variable\": \"transitivity\", \"metric\": \"recall\", \"score\": 0.0}, {\"variable\": \"transitivity\", \"metric\": \"accuracy\", \"score\": 1.0}, {\"variable\": \"transitivity\", \"metric\": \"F1\", \"score\": 0.0}, {\"variable\": \"causativity\", \"metric\": \"precision\", \"score\": 0.0}, {\"variable\": \"causativity\", \"metric\": \"recall\", \"score\": 0.0}, {\"variable\": \"causativity\", \"metric\": \"accuracy\", \"score\": 1.0}, {\"variable\": \"causativity\", \"metric\": \"F1\", \"score\": 0.0}, {\"variable\": \"subject_animacy\", \"metric\": \"precision\", \"score\": 0.0}, {\"variable\": \"subject_animacy\", \"metric\": \"recall\", \"score\": 0.0}, {\"variable\": \"subject_animacy\", \"metric\": \"accuracy\", \"score\": 1.0}, {\"variable\": \"subject_animacy\", \"metric\": \"F1\", \"score\": 0.0}, {\"variable\": \"subject_role\", \"metric\": \"precision\", \"score\": 0.0}, {\"variable\": \"subject_role\", \"metric\": \"recall\", \"score\": 0.0}, {\"variable\": \"subject_role\", \"metric\": \"accuracy\", \"score\": 1.0}, {\"variable\": \"subject_role\", \"metric\": \"F1\", \"score\": 0.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = alt.Chart(metrics).mark_bar().encode(\n",
    "\ty='score:Q',\n",
    "\tx=alt.X('metric:N', sort=metrics['metric'].tolist()),\n",
    "\tcolor=alt.Color('metric', legend=None),\n",
    ").facet(column='variable:N')\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_datetime = datetime.now()\n",
    "# formatted_date = current_datetime.strftime(\"%Y-%m-%d_%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../out/{formatted_date}_prompt.md', 'w') as f_prompt:\n",
    "# \tf_prompt.write(prompt_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../out/{formatted_date}_model.md', 'w') as f_prompt:\n",
    "# \tf_prompt.write(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart.save(f'../out/{formatted_date}_metrics_plot.png', scale=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.to_csv(f'../out/{formatted_date}_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
